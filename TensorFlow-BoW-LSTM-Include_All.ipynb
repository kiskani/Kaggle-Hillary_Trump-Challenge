{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import re\n",
    "import collections \n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data gathering and data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/mohsenkiskani/.kaggle/competitions/cmps242-spring18-hw5/train.csv')\n",
    "test_data = pd.read_csv('/Users/mohsenkiskani/.kaggle/competitions/cmps242-spring18-hw5/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_from_list(A):\n",
    "    if len(A)>0:\n",
    "        B = (\" \".join(A)).lower()\n",
    "        return re.sub('[^a-zA-z0-9\\s]','',B)\n",
    "    else:\n",
    "        return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['hashtags'] =  data['tweet'].apply((lambda x: re.findall(r\"#(\\w+)\", x)))\n",
    "test_data['hashtags'] =  test_data['tweet'].apply((lambda x: re.findall(r\"#(\\w+)\", x)))\n",
    "\n",
    "data['mention'] =  data['tweet'].apply((lambda x: re.findall(r\"#(\\w+)\", x)))\n",
    "test_data['mention'] =  test_data['tweet'].apply((lambda x: re.findall(r\"#(\\w+)\", x)))\n",
    "\n",
    "data['text']  = data['tweet'].apply((lambda x: re.sub(r\"#(\\w+)\", '', x)))\n",
    "data['text']  = data['text'].apply((lambda x: re.sub(r\"@(\\w+)\", '', x)))\n",
    "\n",
    "test_data['text']  = test_data['tweet'].apply((lambda x: re.sub(r\"#(\\w+)\", '', x)))\n",
    "test_data['text']  = test_data['text'].apply((lambda x: re.sub(r\"@(\\w+)\", '', x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_tags = [\"littlemarco\", \"makeamericagreatagain\", \"trump2016\", \"crookedhillary\", \"votetrumpsc\", \n",
    "              \"votetrump\", \"trumppence16\", \"trumppence2016\", \"americafirst\", \"votetrumpma\", \"neverhillary\",\n",
    "              \"trump4president\", \"lightweightsenatormarcorubio\", \"trumpinstagram\", \"americagreatagain\", \n",
    "              \"trump4vets\", \"votetrumpmi\", \"votetrumphi\", \"votetrumpnh\", \"caucusfortrump\", \"trumpforpresident\",\n",
    "              \"makeyoutubegreatagain\", \"votetrumpms\", \"votetrumpil\", \"votetrumpny\", \"votetrumpks\", \"trumppence2016\",\n",
    "             ]\n",
    "\n",
    "trump_list   = []\n",
    "clinton_list = []\n",
    "\n",
    "for i in range(test_data.shape[0]):\n",
    "    test_data['hashtags'][i] = [x.lower() for x in test_data['hashtags'][i]]\n",
    "    \n",
    "    for item in trump_tags:\n",
    "        if item in test_data['hashtags'][i]:\n",
    "            trump_list.append(i)\n",
    "            break\n",
    "        \n",
    "    if \"lovetrumpshate\" in test_data['hashtags'][i] or \"nevertrump\" in test_data['hashtags'][i]:\n",
    "        clinton_list.append(i)\n",
    "        \n",
    "trump_list   = set(trump_list)\n",
    "clinton_list = set(clinton_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hashtags_trump   = []\n",
    "all_hashtags_clinton = []\n",
    "\n",
    "for i in range(data.shape[0]):\n",
    "    data['hashtags'][i] = [x.lower() for x in data['hashtags'][i]]\n",
    "    if data['handle'][i]=='realDonaldTrump':\n",
    "        all_hashtags_trump.extend(data['hashtags'][i])\n",
    "    else:\n",
    "        all_hashtags_clinton.extend(data['hashtags'][i])\n",
    "\n",
    "all_hashtags_trump_set = set(all_hashtags_trump)\n",
    "all_hashtags_clinton_set = set(all_hashtags_clinton)\n",
    "#print(all_hashtags_trump_set)\n",
    "#print(all_hashtags_clinton_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['handle']    = data['handle'].replace({'realDonaldTrump':1, 'HillaryClinton':0})\n",
    "\n",
    "data['tweet']     = data['tweet'].apply((lambda x: re.sub(r\"http\\S+\", 'urlurlurlurl', x)))\n",
    "data['tweet']     = data['tweet'].apply(lambda x: x.lower())\n",
    "data['tweet']     = data['tweet'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "data['tweet']     = data['tweet'].apply((lambda x: re.sub(' +',' ',x)))\n",
    "data['tweet']     = data['tweet'].apply((lambda x: re.sub(r'(\\n+)(?=[A-Z])', r' ', x)))\n",
    "\n",
    "test_data['tweet']     = test_data['tweet'].apply((lambda x: re.sub(r\"http\\S+\", 'urlurlurlurl', x)))\n",
    "test_data['tweet']     = test_data['tweet'].apply(lambda x: x.lower())\n",
    "test_data['tweet']     = test_data['tweet'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "test_data['tweet']     = test_data['tweet'].apply((lambda x: re.sub(' +',' ',x)))\n",
    "test_data['tweet']     = test_data['tweet'].apply((lambda x: re.sub(r'(\\n+)(?=[A-Z])', r' ', x)))\n",
    "\n",
    "\n",
    "all_tweets = pd.concat([data['tweet'], test_data['tweet']])\n",
    "\n",
    "X       = data['tweet']\n",
    "Y       = np.array(data['handle'].tolist())\n",
    "X_test  = test_data['tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of word's model, feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec_hashtag          = CountVectorizer()\n",
    "tfidf_transformer_hashtag  = TfidfTransformer()\n",
    "\n",
    "X_hashtag       = [\" \".join(x) for x in data['hashtags']]\n",
    "X_test_hashtag  = [\" \".join(x) for x in test_data['hashtags']]\n",
    "\n",
    "X_counts_hashtag         = count_vec_hashtag.fit_transform(X_hashtag)\n",
    "X_tfidf_hashtag          = tfidf_transformer_hashtag.fit_transform(X_counts_hashtag)\n",
    "\n",
    "X_test_count_hashtag     = count_vec_hashtag.transform(X_test_hashtag)\n",
    "X_test_tfidf_hashtag     = tfidf_transformer_hashtag.fit_transform(X_test_count_hashtag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf_hashtag_np      = X_tfidf_hashtag.toarray()\n",
    "X_test_tfidf_hashtag_np = X_test_tfidf_hashtag.toarray()\n",
    "\n",
    "X_train_tfidf_hashtag, X_val_tfidf_hashtag, Y_train_tfidf_hashtag, Y_val_tfidf_hashtag = \\\n",
    "                  train_test_split(X_tfidf_hashtag_np, Y, test_size = 0.1)#, random_state = 42)\n",
    "\n",
    "bow_dim_size_hashtag    = X_tfidf_hashtag.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec_mention          = CountVectorizer()\n",
    "tfidf_transformer_mention  = TfidfTransformer()\n",
    "\n",
    "X_mention       = [\" \".join(x) for x in data['mention']]\n",
    "X_test_mention  = [\" \".join(x) for x in test_data['mention']]\n",
    "\n",
    "X_counts_mention         = count_vec_mention.fit_transform(X_mention)\n",
    "X_tfidf_mention          = tfidf_transformer_mention.fit_transform(X_counts_mention)\n",
    "\n",
    "X_test_count_mention     = count_vec_mention.transform(X_test_mention)\n",
    "X_test_tfidf_mention     = tfidf_transformer_mention.fit_transform(X_test_count_mention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf_mention_np      = X_tfidf_mention.toarray()\n",
    "X_test_tfidf_mention_np = X_test_tfidf_mention.toarray()\n",
    "X_train_tfidf_mention, X_val_tfidf_mention, Y_train_tfidf_mention, Y_val_tfidf_mention = \\\n",
    "                  train_test_split(X_tfidf_mention_np, Y, test_size = 0.1)#, random_state = 42)\n",
    "bow_dim_size_mention    = X_tfidf_mention.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec         = CountVectorizer()\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "X_train_bow_pd, X_val_bow_pd, Y_train_bow_pd, Y_val_bow_pd = train_test_split(data['text'], Y,\\\n",
    "                                                                              test_size=0.1)#, random_state = 42)\n",
    "\n",
    "X_train_counts    = count_vec.fit_transform(X_train_bow_pd)\n",
    "X_train_tfidf     = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "X_val_counts      = count_vec.transform(X_val_bow_pd)\n",
    "X_val_tfidf       = tfidf_transformer.transform(X_val_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperVec                  = [1e6,1e5,1e4,1e3,1e2,1e1,1e0,1e-1,1e-2]\n",
    "#loss_dic_bow              = {}\n",
    "\n",
    "#for C in hyperVec:\n",
    "#    lf_tmp = LogisticRegression(C=C)\n",
    "#    lf_tmp.fit(X_train_tfidf, Y_train_bow_pd)\n",
    "#    y_val_predicted_tmp = lf_tmp.predict(X_val_tfidf)\n",
    "#    loss_dic_bow[C] = log_loss(y_pred = y_val_predicted_tmp, y_true =  Y_val_bow_pd)\n",
    "   \n",
    "#C_optimal = min(loss_dic_bow, key=loss_dic_bow.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_counts           = count_vec.fit_transform(X)\n",
    "X_tfidf            = tfidf_transformer.fit_transform(X_counts)\n",
    "\n",
    "X_test_counts      = count_vec.transform(X_test)\n",
    "X_test_tfidf       = tfidf_transformer.transform(X_test_counts)\n",
    "\n",
    "#lf = LogisticRegression(C = C_optimal)\n",
    "#lf.fit(X_tfidf, Y)\n",
    "#Y_bow_predicted_test = lf.predict(X_test_tfidf)\n",
    "\n",
    "#print(len(Y_bow_predicted_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf_np      = X_tfidf.toarray()\n",
    "X_test_tfidf_np = X_test_tfidf.toarray()\n",
    "X_train_tfidf, X_val_tfidf, Y_train_tfidf, Y_val_tfidf = train_test_split(X_tfidf_np,\\\n",
    "                                                                          Y, test_size = 0.1)#, random_state = 42)\n",
    "bow_dim_size    = X_tfidf.shape[1]\n",
    "#print(X_test_tfidf_np.shape)\n",
    "#print(X_tfidf_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = []\n",
    "for i in range(X.shape[0]):\n",
    "    X_all.append(X[i].split())\n",
    "    \n",
    "for i in range(X_test.shape[0]):\n",
    "    X_all.append(X_test[i].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets         = all_tweets.tolist()\n",
    "words          = \" \".join(tweets)\n",
    "words          = \" \".join(words.split(\"\\n\")) \n",
    "words          = \" \".join(words.split(\"\\t\"))\n",
    "words          = \" \".join(words.split(\"\\xa0\")).split()\n",
    "whole_text     = \" \".join(words)\n",
    "\n",
    "count          = collections.Counter(words)\n",
    "count_clean    = [(item, count[item]) for item in count if count[item]>1]\n",
    "vocab_size     = len(count_clean)\n",
    "num_words      = vocab_size+2 \n",
    "\n",
    "dic = {}\n",
    "\n",
    "for i in range(vocab_size):\n",
    "    dic[count_clean[i][0]] = (i+1, count_clean[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_token_list(A):\n",
    "    B      = A.strip().split(\" \")\n",
    "    output = []\n",
    "    for item in B:\n",
    "        if item in dic:\n",
    "            output.append(dic[item][0])\n",
    "        else:\n",
    "            output.append(vocab_size + 1)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tokens       = X.apply(lambda x: string_to_token_list(x))\n",
    "X_test_tokens  = X_test.apply(lambda x: string_to_token_list(x)) \n",
    "\n",
    "num_tokens     = [len(tokens) for tokens in X_tokens]\n",
    "num_tokens     = np.array(num_tokens)\n",
    "max_tokens     = int(np.max(num_tokens))\n",
    "\n",
    "X_pad          = []\n",
    "X_test_pad     = []\n",
    "\n",
    "for item in X_tokens:\n",
    "    n = len(item)\n",
    "    X_pad.append([0]*(max_tokens-n) + item)\n",
    "\n",
    "for item in X_test_tokens:\n",
    "    n = len(item)\n",
    "    X_test_pad.append([0]*(max_tokens-n) + item)\n",
    "    \n",
    "X_pad        = np.array(X_pad)\n",
    "X_test_pad   = np.array(X_test_pad)\n",
    "X_all_pad    = np.concatenate((X_pad , X_test_pad), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pad, X_val_pad, Y_train_pad, Y_val_pad = train_test_split(X_pad, Y, test_size = 0.1)#, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameter selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr                         = 1e-5\n",
    "\n",
    "lstm_dropout_prob          = 0.5\n",
    "lstm_forget_bias           = 1.0\n",
    "reg_const                  = 100.0\n",
    "reg_const_bow              = 10.0\n",
    "\n",
    "embedding_size             = 64\n",
    "batch_size                 = 32\n",
    "lstm_out                   = 32\n",
    "\n",
    "bow_hidden_layer_1_size    = 128\n",
    "bow_hidden_layer_2_size    = 32\n",
    "\n",
    "hashtag_hidden_layer_size  = 32\n",
    "mention_hidden_layer_size  = 32\n",
    "\n",
    "hidden_layer_1_size        = lstm_out + bow_hidden_layer_2_size + hashtag_hidden_layer_size + mention_hidden_layer_size\n",
    "hidden_layer_2_size        = 64 #lstm_out + bow_hidden_layer_2_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(X_all, iter=100, min_count=2, size=embedding_size, workers=4)\n",
    "#words = list(model.wv.vocab)\n",
    "#print(words)\n",
    "#print(model.wv.similar_by_word('makeamericagreatagain'))\n",
    "#model.wv['trump'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((1,embedding_size), np.float32)\n",
    "\n",
    "for i in range(vocab_size):\n",
    "    word             = count_clean[i][0]\n",
    "    word_vec         = model.wv[word]\n",
    "    embedding_matrix = np.vstack((embedding_matrix, word_vec))\n",
    "\n",
    "embedding_matrix = np.vstack((embedding_matrix, 5* np.ones((1,embedding_size), np.float32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test_data_set(filename, X_test_pad, X_test_tfidf_np, X_test_tfidf_hashtag_np, X_test_tfidf_mention_np, sess):\n",
    "        test_size   = X_test_pad.shape[0]\n",
    "        test_remain = batch_size - (test_size % batch_size) \n",
    "        X_test_pad      = np.concatenate((X_test_pad, np.zeros((test_remain, max_tokens), np.float32)), axis=0)\n",
    "        X_test_tfidf_np = np.concatenate((X_test_tfidf_np, np.zeros((test_remain, bow_dim_size), np.float32)), axis=0)\n",
    "        X_test_tfidf_hashtag_np = np.concatenate((X_test_tfidf_hashtag_np, \\\n",
    "                                                  np.zeros((test_remain, bow_dim_size_hashtag), np.float32)), axis=0)\n",
    "        X_test_tfidf_mention_np = np.concatenate((X_test_tfidf_mention_np, \\\n",
    "                                                  np.zeros((test_remain, bow_dim_size_mention), np.float32)), axis=0)\n",
    "        \n",
    "        m = test_size // batch_size \n",
    "        pred_test_vals = np.empty((0, 2), np.float32)\n",
    "\n",
    "        for i in range(m+1):\n",
    "            input_test_batch              = X_test_pad[i * batch_size : (i+1) * batch_size, :]\n",
    "            input_X_test_tfidf            = X_test_tfidf_np[i * batch_size : (i+1) * batch_size, :]\n",
    "            input_X_test_tfidf_hashtag    = X_test_tfidf_hashtag_np[i * batch_size : (i+1) * batch_size, :]\n",
    "            input_X_test_tfidf_mention    = X_test_tfidf_mention_np[i * batch_size : (i+1) * batch_size, :]\n",
    "            test_preds_list  = sess.run([predictions], feed_dict = {X_bow     : input_X_test_tfidf,\n",
    "                                                                    X_hashtag : input_X_test_tfidf_hashtag,\n",
    "                                                                    X_mention : input_X_test_tfidf_mention,\n",
    "                                                                    X         : input_test_batch})\n",
    "            pred_test_batch  = np.asarray(test_preds_list).reshape(batch_size, 2)\n",
    "            pred_test_vals   = np.concatenate((pred_test_vals, pred_test_batch), axis=0)\n",
    "            \n",
    "        with open(filename,\"w+\") as outputfile:\n",
    "            outputfile.write(\"id,realDonaldTrump,HillaryClinton\\n\")\n",
    "            for j in range(test_size):\n",
    "                #if j in trump_list:\n",
    "                #    hillary = 0 \n",
    "                #    donald  = 1 \n",
    "                #elif j in clinton_list:\n",
    "                #    hillary = 1 \n",
    "                #    donald  = 0\n",
    "                #else:\n",
    "                hillary = pred_test_vals[j][1]\n",
    "                donald  = pred_test_vals[j][0]\n",
    "                outputfile.write(str(j)+\",\"+str(donald)+\",\"+str(hillary)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(X, Y, X_tfidf, X_tfidf_hashtag, X_tfidf_mention, size):\n",
    "    rdm         = np.random.choice(X.shape[0], size , replace = False)\n",
    "    y1          = Y[rdm].reshape((size,1))\n",
    "    y2          = (y1+1)%2\n",
    "    Y_out       = np.concatenate((y1, y2), axis=1)\n",
    "    X_out       = X[rdm,:]\n",
    "    X_tfidf_out = X_tfidf[rdm,:] \n",
    "    X_tfidf_hashtag_out = X_tfidf_hashtag[rdm,:]\n",
    "    X_tfidf_mention_out = X_tfidf_mention[rdm,:]\n",
    "    return X_tfidf_mention_out, X_tfidf_hashtag_out, X_tfidf_out, X_out, Y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at \n",
    "https://github.com/bernhard2202/twitter-sentiment-analysis/blob/master/model/lstm.py\n",
    "\n",
    "For a similar implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/cpu:0\"):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(0)\n",
    "\n",
    "    X_bow     = tf.placeholder(tf.float32, shape=[batch_size, bow_dim_size])\n",
    "    X_hashtag = tf.placeholder(tf.float32, shape=[batch_size, bow_dim_size_hashtag])\n",
    "    X_mention = tf.placeholder(tf.float32, shape=[batch_size, bow_dim_size_mention])\n",
    "    X         = tf.placeholder(tf.int32, shape=[batch_size, max_tokens])\n",
    "    Y         = tf.placeholder(tf.float32, shape=[batch_size,2])\n",
    "        \n",
    "    embeddings     = tf.Variable(embedding_matrix)    \n",
    "    embed          = tf.nn.embedding_lookup(embeddings, X)\n",
    "    embed          = tf.unstack(embed, max_tokens, 1)\n",
    "    \n",
    "    lstm_cell                 = tf.contrib.rnn.BasicLSTMCell(lstm_out, forget_bias = lstm_forget_bias)\n",
    "    lstm_cell                 = tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=lstm_dropout_prob)\n",
    "    lstm_output , lstm_state  = tf.contrib.rnn.static_rnn(lstm_cell, inputs=embed , dtype=tf.float32)\n",
    "    \n",
    "    outputs                   = tf.stack(lstm_output)\n",
    "    outputs                   = tf.gather(outputs, max_tokens-1, axis=0)\n",
    "    outputs                   = tf.reshape(outputs, [batch_size, lstm_out])\n",
    "    lstm_final_output         = tf.reshape(outputs, [batch_size , lstm_out])\n",
    "    \n",
    "    bow_hidden_layer_1_output = tf.layers.dense(X_bow, bow_hidden_layer_1_size, \n",
    "                                               kernel_regularizer = tf.contrib.layers.l2_regularizer(reg_const_bow))\n",
    "    bow_hidden_layer_2_output = tf.layers.dense(bow_hidden_layer_1_output, bow_hidden_layer_2_size, \n",
    "                                               kernel_regularizer = tf.contrib.layers.l2_regularizer(reg_const_bow))\n",
    "    \n",
    "    hashtag_hidden_layer      = tf.layers.dense(X_hashtag, hashtag_hidden_layer_size, \n",
    "                                               kernel_regularizer = tf.contrib.layers.l2_regularizer(reg_const_bow))\n",
    "    \n",
    "    mention_hidden_layer      = tf.layers.dense(X_mention, mention_hidden_layer_size, \n",
    "                                               kernel_regularizer = tf.contrib.layers.l2_regularizer(reg_const_bow))\n",
    "    \n",
    "    bow_concat_lstm           = tf.concat( [lstm_final_output, X_bow, X_hashtag, X_mention], axis=1)\n",
    "    \n",
    "    hidden_layer_1_output     = tf.layers.dense(bow_concat_lstm, hidden_layer_1_size,\n",
    "                                                kernel_regularizer = tf.contrib.layers.l2_regularizer(reg_const) )\n",
    "    hidden_layer_2_output     = tf.layers.dense(hidden_layer_1_output, hidden_layer_2_size,\n",
    "                                                kernel_regularizer = tf.contrib.layers.l2_regularizer(reg_const))\n",
    "    \n",
    "    out_weight      = tf.Variable(tf.random_normal([hidden_layer_2_size, 2]))\n",
    "    out_bias        = tf.Variable(tf.random_normal([2]))\n",
    "    \n",
    "    scores          = tf.nn.xw_plus_b(hidden_layer_2_output, out_weight,out_bias)\n",
    "    predictions     = tf.nn.softmax(scores)\n",
    "    \n",
    "    losses          = tf.nn.softmax_cross_entropy_with_logits_v2(logits = scores,\n",
    "                                                              labels = Y)\n",
    "    loss            = tf.reduce_mean(losses)\n",
    "    \n",
    "    correct_pred    = tf.equal(tf.argmax(predictions, 1), tf.argmax(Y, 1))\n",
    "    accuracy        = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    optimizer       = tf.train.AdamOptimizer(lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch loss =  0.852,  training accuray =  0.625,  val loss =  1.115,  val accuray =  0.438, \n",
      "Epoch 100, Batch loss =  0.937,  training accuray =  0.500,  val loss =  1.118,  val accuray =  0.344, \n",
      "Epoch 200, Batch loss =  0.839,  training accuray =  0.438,  val loss =  0.810,  val accuray =  0.500, \n",
      "Epoch 300, Batch loss =  0.698,  training accuray =  0.562,  val loss =  1.032,  val accuray =  0.375, \n",
      "Epoch 400, Batch loss =  0.557,  training accuray =  0.750,  val loss =  0.821,  val accuray =  0.438, \n",
      "Epoch 500, Batch loss =  0.734,  training accuray =  0.469,  val loss =  0.650,  val accuray =  0.688, \n",
      "Epoch 600, Batch loss =  0.661,  training accuray =  0.625,  val loss =  0.651,  val accuray =  0.625, \n",
      "Epoch 700, Batch loss =  0.515,  training accuray =  0.812,  val loss =  0.640,  val accuray =  0.562, \n",
      "Epoch 800, Batch loss =  0.615,  training accuray =  0.750,  val loss =  0.600,  val accuray =  0.688, \n",
      "Epoch 900, Batch loss =  0.526,  training accuray =  0.719,  val loss =  0.442,  val accuray =  0.781, \n",
      "Epoch 1000, Batch loss =  0.589,  training accuray =  0.688,  val loss =  0.540,  val accuray =  0.656, \n",
      "Epoch 1100, Batch loss =  0.442,  training accuray =  0.812,  val loss =  0.488,  val accuray =  0.750, \n",
      "Epoch 1200, Batch loss =  0.476,  training accuray =  0.781,  val loss =  0.374,  val accuray =  0.906, \n",
      "Epoch 1300, Batch loss =  0.580,  training accuray =  0.656,  val loss =  0.495,  val accuray =  0.750, \n",
      "Epoch 1400, Batch loss =  0.426,  training accuray =  0.844,  val loss =  0.434,  val accuray =  0.781, \n",
      "Epoch 1500, Batch loss =  0.486,  training accuray =  0.750,  val loss =  0.404,  val accuray =  0.812, \n",
      "Epoch 1600, Batch loss =  0.371,  training accuray =  0.875,  val loss =  0.590,  val accuray =  0.688, \n",
      "Epoch 1700, Batch loss =  0.456,  training accuray =  0.719,  val loss =  0.552,  val accuray =  0.750, \n",
      "Epoch 1800, Batch loss =  0.337,  training accuray =  0.844,  val loss =  0.445,  val accuray =  0.750, \n",
      "Epoch 1900, Batch loss =  0.433,  training accuray =  0.844,  val loss =  0.298,  val accuray =  0.844, \n",
      "Epoch 2000, Batch loss =  0.330,  training accuray =  0.875,  val loss =  0.403,  val accuray =  0.750, \n",
      "Epoch 2100, Batch loss =  0.446,  training accuray =  0.750,  val loss =  0.374,  val accuray =  0.812, \n",
      "Epoch 2200, Batch loss =  0.300,  training accuray =  0.906,  val loss =  0.605,  val accuray =  0.656, \n",
      "Epoch 2300, Batch loss =  0.420,  training accuray =  0.812,  val loss =  0.427,  val accuray =  0.844, \n",
      "Epoch 2400, Batch loss =  0.300,  training accuray =  0.906,  val loss =  0.366,  val accuray =  0.906, \n",
      "Epoch 2500, Batch loss =  0.438,  training accuray =  0.812,  val loss =  0.405,  val accuray =  0.781, \n",
      "Epoch 2600, Batch loss =  0.314,  training accuray =  0.844,  val loss =  0.624,  val accuray =  0.656, \n",
      "Epoch 2700, Batch loss =  0.365,  training accuray =  0.875,  val loss =  0.330,  val accuray =  0.875, \n",
      "Epoch 2800, Batch loss =  0.205,  training accuray =  0.938,  val loss =  0.512,  val accuray =  0.719, \n",
      "Epoch 2900, Batch loss =  0.319,  training accuray =  0.781,  val loss =  0.438,  val accuray =  0.781, \n",
      "Epoch 3000, Batch loss =  0.289,  training accuray =  0.844,  val loss =  0.381,  val accuray =  0.781, \n",
      "Epoch 3100, Batch loss =  0.320,  training accuray =  0.844,  val loss =  0.346,  val accuray =  0.875, \n",
      "Epoch 3200, Batch loss =  0.399,  training accuray =  0.844,  val loss =  0.527,  val accuray =  0.750, \n",
      "Epoch 3300, Batch loss =  0.215,  training accuray =  0.906,  val loss =  0.393,  val accuray =  0.844, \n",
      "Epoch 3400, Batch loss =  0.452,  training accuray =  0.812,  val loss =  0.357,  val accuray =  0.844, \n",
      "Epoch 3500, Batch loss =  0.325,  training accuray =  0.844,  val loss =  0.331,  val accuray =  0.844, \n",
      "Epoch 3600, Batch loss =  0.160,  training accuray =  0.969,  val loss =  0.377,  val accuray =  0.875, \n",
      "Epoch 3700, Batch loss =  0.375,  training accuray =  0.844,  val loss =  0.419,  val accuray =  0.844, \n",
      "Epoch 3800, Batch loss =  0.275,  training accuray =  0.844,  val loss =  0.372,  val accuray =  0.844, \n",
      "Epoch 3900, Batch loss =  0.275,  training accuray =  0.875,  val loss =  0.359,  val accuray =  0.812, \n",
      "Epoch 4000, Batch loss =  0.266,  training accuray =  0.969,  val loss =  0.257,  val accuray =  0.875, \n",
      "Epoch 4100, Batch loss =  0.215,  training accuray =  0.938,  val loss =  0.273,  val accuray =  0.875, \n",
      "Epoch 4200, Batch loss =  0.185,  training accuray =  0.938,  val loss =  0.332,  val accuray =  0.844, \n",
      "Epoch 4300, Batch loss =  0.273,  training accuray =  0.812,  val loss =  0.291,  val accuray =  0.875, \n",
      "Epoch 4400, Batch loss =  0.286,  training accuray =  0.938,  val loss =  0.335,  val accuray =  0.844, \n",
      "Epoch 4500, Batch loss =  0.249,  training accuray =  0.906,  val loss =  0.325,  val accuray =  0.875, \n"
     ]
    }
   ],
   "source": [
    "epochs           = 8000\n",
    "display_epoch    = 100\n",
    "\n",
    "import time\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "allstr = \"batchSize_\" + str(batch_size) + \"_embeddingSize_\" + str(embedding_size) + \\\n",
    "         \"_lstmOut_\" + str(lstm_out) + \"_bowSize2_\" + str(bow_hidden_layer_2_size) + \\\n",
    "         \"_hidden1_\" + str(hidden_layer_1_size) + \"_hidden2_\" + str(hidden_layer_2_size) + \"_time_\"+ timestr\n",
    "filename = \"/Users/mohsenkiskani/Downloads/submissions/structred/lstm_bow_\"+str(allstr)+\".csv\"\n",
    "\n",
    "loss_val_vec     = []\n",
    "accur_val_vec    = []\n",
    "loss_train_vec   = []\n",
    "accur_train_vec  = []\n",
    "epoch_vec        = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            batch_mention_vals, batch_hashtag_vals, batch_bow_vals, batch_inputs, batch_labels = get_batch(X_train_pad,\n",
    "                                                                                                           Y_train_pad,\n",
    "                                                                                                           X_train_tfidf,\n",
    "                                                                                                           X_train_tfidf_hashtag,\n",
    "                                                                                                           X_train_tfidf_mention,\n",
    "                                                                                                           batch_size)\n",
    "            loss_train, accur_train, _ = sess.run([loss, accuracy, optimizer],\n",
    "                                                  feed_dict = {X_bow     : batch_bow_vals,\n",
    "                                                               X_hashtag : batch_hashtag_vals,\n",
    "                                                               X_mention : batch_mention_vals,\n",
    "                                                               X         : batch_inputs,\n",
    "                                                               Y         : batch_labels})\n",
    "            \n",
    "            batch_mention_vals, batch_hashtag_vals, batch_bow_vals, batch_inputs, batch_labels = get_batch(X_val_pad,\n",
    "                                                                                                           Y_val_pad,\n",
    "                                                                                                           X_val_tfidf,\n",
    "                                                                                                           X_val_tfidf_hashtag,\n",
    "                                                                                                           X_val_tfidf_mention,\n",
    "                                                                                                           batch_size)\n",
    "            \n",
    "            loss_val, accur_val = sess.run([loss, accuracy],\n",
    "                                           feed_dict = {X_bow     : batch_bow_vals,\n",
    "                                                        X_hashtag : batch_hashtag_vals,\n",
    "                                                        X_mention : batch_mention_vals,\n",
    "                                                        X         : batch_inputs,\n",
    "                                                        Y         : batch_labels})\n",
    "                \n",
    "            loss_train_vec.append(loss_train)\n",
    "            accur_train_vec.append(accur_train)\n",
    "            loss_val_vec.append(loss_val)\n",
    "            accur_val_vec.append(accur_val)\n",
    "            epoch_vec.append(epoch)\n",
    "            \n",
    "            if epoch % display_epoch == 0:\n",
    "                print(\"Epoch \" +str(epoch) + \", Batch loss = \", \n",
    "                      \"%.3f, \" % loss_train , \"training accuray = \",  \"%.3f, \" % accur_train, \"val loss = \",\n",
    "                      \"%.3f, \" % loss_val , \"val accuray = \",  \"%.3f, \" % accur_val)\n",
    "        \n",
    "        evaluate_test_data_set(filename, X_test_pad, X_test_tfidf_np, X_test_tfidf_hashtag_np, X_test_tfidf_mention_np, sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plotting_parameter = 300\n",
    "\n",
    "const_vec_02 = [0.2]* len(epoch_vec)\n",
    "const_vec_01 = [0.1]* len(epoch_vec)\n",
    "\n",
    "fig1, (ay1, ay2) = plt.subplots(2, 1)\n",
    "ay1.plot(epoch_vec[plotting_parameter:], loss_train_vec[plotting_parameter:])\n",
    "ay1.plot(epoch_vec[plotting_parameter:], const_vec_01[plotting_parameter:])\n",
    "ay1.plot(epoch_vec[plotting_parameter:], const_vec_02[plotting_parameter:])\n",
    "\n",
    "ay2.plot(epoch_vec[plotting_parameter:], loss_val_vec[plotting_parameter:])\n",
    "ay2.plot(epoch_vec[plotting_parameter:], const_vec_01[plotting_parameter:])\n",
    "ay2.plot(epoch_vec[plotting_parameter:], const_vec_02[plotting_parameter:])\n",
    "\n",
    "\n",
    "fig2, (ay3, ay4) = plt.subplots(2, 1)\n",
    "ay3.plot(epoch_vec[plotting_parameter:], accur_train_vec[plotting_parameter:])\n",
    "#ay3.plot(epoch_vec[plotting_parameter:], const_vec[plotting_parameter:])\n",
    "ay4.plot(epoch_vec[plotting_parameter:], accur_val_vec[plotting_parameter:])\n",
    "#ay4.plot(epoch_vec[plotting_parameter:], const_vec[plotting_parameter:])\n",
    "\n",
    "\n",
    "ay1.set_title('Training batch loss vs epoch')\n",
    "ay1.set_xlabel('Epoch')\n",
    "ay1.set_ylabel('Training batch loss')\n",
    "\n",
    "ay2.set_title('Validation batch loss vs epoch')\n",
    "ay2.set_xlabel('Epoch')\n",
    "ay2.set_ylabel('Validation batch loss')\n",
    "\n",
    "ay3.set_title('Training batch accuracy vs epoch')\n",
    "ay3.set_xlabel('Epoch')\n",
    "ay3.set_ylabel('Train accuracy')\n",
    "\n",
    "ay4.set_title('Validation batch accuracy vs epoch')\n",
    "ay4.set_xlabel('Epoch')\n",
    "ay4.set_ylabel('Validation accuracy')\n",
    "\n",
    "fig1.set_size_inches(18.5, 10.5)\n",
    "fig1.savefig(\"/Users/mohsenkiskani/Downloads/submissions/structred/loss_\"+timestr+\".png\", dpi=100)\n",
    "\n",
    "fig2.set_size_inches(18.5, 10.5)\n",
    "fig2.savefig(\"/Users/mohsenkiskani/Downloads/submissions/structred/accuracy_\"+timestr+\".png\", dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
