{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import re\n",
    "import collections \n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data gathering and data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/mohsenkiskani/.kaggle/competitions/cmps242-spring18-hw5/train.csv')\n",
    "test_data = pd.read_csv('/Users/mohsenkiskani/.kaggle/competitions/cmps242-spring18-hw5/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_data(A):\n",
    "    BB = A.strip().split(\"\\n\")\n",
    "    B = []\n",
    "    for line in BB:\n",
    "        B.extend(line.strip().split(\" \"))\n",
    "    handles  = []\n",
    "    hashtags = []\n",
    "    for item in B:\n",
    "        i = 0\n",
    "        while (i < len(item)):\n",
    "            if item[i]=='@':\n",
    "                handle = item[i+1:] \n",
    "                #handle = re.sub('[^a-zA-z0-9\\s]','',handle)\n",
    "                #handle = handle.lower()\n",
    "                handles.append(handle)\n",
    "                B.remove(item)\n",
    "                break\n",
    "            elif item[i]=='#':\n",
    "                hashtag = item[i+1:] \n",
    "                #hashtag = re.sub('[^a-zA-z0-9\\s]','',hashtag)\n",
    "                #hashtag = hashtag.lower()\n",
    "                hashtags.append(hashtag)\n",
    "                B.remove(item)\n",
    "                break\n",
    "            else: \n",
    "                i += 1 \n",
    "    text = \" \".join(B)\n",
    "    #text = re.sub('[^a-zA-z0-9\\s]','',text)\n",
    "    #text = text.lower()\n",
    "    return handles, hashtags, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_from_list(A):\n",
    "    if len(A)>0:\n",
    "        B = (\" \".join(A)).lower()\n",
    "        return re.sub('[^a-zA-z0-9\\s]','',B)\n",
    "    else:\n",
    "        return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['handle']    = data['handle'].replace({'realDonaldTrump':1, 'HillaryClinton':0})\n",
    "\n",
    "data['tweet']     = data['tweet'].apply((lambda x: re.sub(r\"http\\S+\", 'urlurlurlurl', x)))\n",
    "\n",
    "data['mentioned'] = data['tweet'].apply((lambda x: get_clean_data(x)[0]))\n",
    "data['mentioned'] = data['mentioned'].apply((lambda x: string_from_list(x)))\n",
    "\n",
    "data['hashtags']  = data['tweet'].apply((lambda x: get_clean_data(x)[1]))\n",
    "data['hashtags']  = data['hashtags'].apply((lambda x: string_from_list(x) ))\n",
    "\n",
    "data['tweet']     = data['tweet'].apply(lambda x: x.lower())\n",
    "data['tweet']     = data['tweet'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "data['tweet']     = data['tweet'].apply((lambda x: re.sub(' +',' ',x)))\n",
    "data['tweet']     = data['tweet'].apply((lambda x: re.sub(r'(\\n+)(?=[A-Z])', r' ', x)))\n",
    "\n",
    "\n",
    "test_data['tweet']     = test_data['tweet'].apply((lambda x: re.sub(r\"http\\S+\", 'urlurlurlurl', x)))\n",
    "\n",
    "test_data['mentioned'] = test_data['tweet'].apply((lambda x: get_clean_data(x)[0]))\n",
    "test_data['mentioned'] = test_data['mentioned'].apply((lambda x: string_from_list(x) ))\n",
    "\n",
    "test_data['hashtags']  = test_data['tweet'].apply((lambda x: get_clean_data(x)[1]))\n",
    "test_data['hashtags']  = test_data['hashtags'].apply((lambda x: string_from_list(x) ))\n",
    "\n",
    "test_data['tweet']     = test_data['tweet'].apply(lambda x: x.lower())\n",
    "test_data['tweet']     = test_data['tweet'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "test_data['tweet']     = test_data['tweet'].apply((lambda x: re.sub(' +',' ',x)))\n",
    "test_data['tweet']     = test_data['tweet'].apply((lambda x: re.sub(r'(\\n+)(?=[A-Z])', r' ', x)))\n",
    "\n",
    "all_tweets = pd.concat([data['tweet'], test_data['tweet']])\n",
    "\n",
    "X       = data['tweet']\n",
    "Y       = np.array(data['handle'].tolist())\n",
    "X_test  = test_data['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hashtag             = data.drop(['tweet', 'mentioned'], axis=1)\n",
    "data_hashtag_clean       = data_hashtag.dropna() \n",
    "\n",
    "X_hashtag_bow_pd         = data_hashtag_clean['hashtags']\n",
    "Y_hashtag_bow_pd         = data_hashtag_clean['handle']\n",
    "\n",
    "X_train_hashtag_bow_pd, X_val_hashtag_bow_pd, Y_train_hashtag_bow_pd, Y_val_hashtag_bow_pd = \\\n",
    "train_test_split(X_hashtag_bow_pd, Y_hashtag_bow_pd,  test_size=0.1)\n",
    "\n",
    "data_hashtag_test        = test_data.drop(['tweet', 'mentioned'], axis=1)\n",
    "data_hashtag_clean_test  = data_hashtag_test.dropna() \n",
    "X_test_hashtag_bow_pd    = data_hashtag_clean_test['hashtags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mentioned             = data.drop(['tweet', 'hashtags'], axis=1)\n",
    "data_mentioned_clean       = data_mentioned.dropna() \n",
    "\n",
    "X_mentioned_bow_pd         = data_mentioned_clean['mentioned']\n",
    "Y_mentioned_bow_pd         = data_mentioned_clean['handle']\n",
    "\n",
    "X_train_mentioned_bow_pd, X_val_mentioned_bow_pd, Y_train_mentioned_bow_pd, Y_val_mentioned_bow_pd = \\\n",
    "train_test_split(X_mentioned_bow_pd, Y_mentioned_bow_pd,  test_size=0.1)\n",
    "\n",
    "data_mentioned_test        = test_data.drop(['tweet', 'hashtags'], axis=1)\n",
    "data_mentioned_clean_test  = data_mentioned_test.dropna() \n",
    "X_test_mentioned_bow_pd    = data_mentioned_clean_test['mentioned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "0               makeamericagreatagain\n",
      "4                       newyorkvalues\n",
      "7                        decision2016\n",
      "8                              diwali\n",
      "10    trump2016 makeamericagreatagain\n",
      "11                               lesm\n",
      "12                          womancard\n",
      "Name: hashtags, dtype: object \n",
      "\n",
      "\n",
      "    handle                                              tweet mentioned  \\\n",
      "11       1  thank you to our law enforcement officers lesm...      None   \n",
      "\n",
      "   hashtags  \n",
      "11     lesm  \n"
     ]
    }
   ],
   "source": [
    "print(X_hashtag_bow_pd.index[5])\n",
    "print(X_hashtag_bow_pd.head(7), \"\\n\\n\")\n",
    "print(data.iloc[[X_hashtag_bow_pd.index[5]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training bag of words models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec_hashtag         = CountVectorizer()\n",
    "tfidf_transformer_hashtag = TfidfTransformer()\n",
    "\n",
    "X_train_counts_hashtag    = count_vec_hashtag.fit_transform(X_train_hashtag_bow_pd)\n",
    "X_train_tfidf_hashtag     = tfidf_transformer_hashtag.fit_transform(X_train_counts_hashtag)\n",
    "\n",
    "X_val_counts_hashtag      = count_vec_hashtag.transform(X_val_hashtag_bow_pd)\n",
    "X_val_tfidf_hashtag       = tfidf_transformer_hashtag.transform(X_val_counts_hashtag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperVec                  = [1e6,1e5,1e4,1e3,1e2,1e1,1e0,1e-1,1e-2]\n",
    "loss_dic_bow_hashtag      = {}\n",
    "\n",
    "for C in hyperVec:\n",
    "    lf_hashtag_tmp = LogisticRegression(C=C)\n",
    "    lf_hashtag_tmp.fit(X_train_tfidf_hashtag, Y_train_hashtag_bow_pd)\n",
    "    y_val_predicted_hashtag_tmp = lf_hashtag_tmp.predict(X_val_tfidf_hashtag)\n",
    "    \n",
    "    loss_dic_bow_hashtag[C] = log_loss(y_pred = y_val_predicted_hashtag_tmp, y_true =  Y_val_hashtag_bow_pd)\n",
    "\n",
    "    \n",
    "C_optimal_hashtag = 1\n",
    "#min(loss_dic_bow_hashtag, key=loss_dic_bow_hashtag.get)\n",
    "#loss_dic_bow_hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_counts_hashtag          = count_vec_hashtag.fit_transform(X_hashtag_bow_pd)\n",
    "X_tfidf_hashtag           = tfidf_transformer_hashtag.fit_transform(X_counts_hashtag)\n",
    "\n",
    "X_test_hashtag_counts     = count_vec_hashtag.transform(X_test_hashtag_bow_pd)\n",
    "X_test_hashtag_tfidf      = tfidf_transformer_hashtag.transform(X_test_hashtag_counts)\n",
    "\n",
    "lf_hashtag = LogisticRegression(C=C_optimal_hashtag)\n",
    "lf_hashtag.fit(X_tfidf_hashtag, Y_hashtag_bow_pd)\n",
    "\n",
    "Y_hashtag_bow_predicted_test = lf_hashtag.predict(X_test_hashtag_tfidf)\n",
    "\n",
    "#print(Y_hashtag_bow_predicted_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec_mentioned          = CountVectorizer()\n",
    "tfidf_transformer_mentioned  = TfidfTransformer()\n",
    "\n",
    "X_train_counts_mentioned    = count_vec_mentioned.fit_transform(X_train_mentioned_bow_pd)\n",
    "X_train_tfidf_mentioned     = tfidf_transformer_mentioned.fit_transform(X_train_counts_mentioned)\n",
    "\n",
    "X_val_counts_mentioned      = count_vec_mentioned.transform(X_val_mentioned_bow_pd)\n",
    "X_val_tfidf_mentioned       = tfidf_transformer_mentioned.transform(X_val_counts_mentioned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperVec                  = [1e6,1e5,1e4,1e3,1e2,1e1,1e0,1e-1,1e-2]\n",
    "loss_dic_bow_mentioned      = {}\n",
    "\n",
    "for C in hyperVec:\n",
    "    lf_mentioned_tmp = LogisticRegression(C=C)\n",
    "    lf_mentioned_tmp.fit(X_train_tfidf_mentioned, Y_train_mentioned_bow_pd)\n",
    "    y_val_predicted_mentioned_tmp = lf_mentioned_tmp.predict(X_val_tfidf_mentioned)\n",
    "    \n",
    "    loss_dic_bow_mentioned[C] = log_loss(y_pred = y_val_predicted_mentioned_tmp, y_true =  Y_val_mentioned_bow_pd)\n",
    "\n",
    "    \n",
    "C_optimal_mentioned = 5\n",
    "#min(loss_dic_bow_mentioned, key=loss_dic_bow_mentioned.get)\n",
    "#loss_dic_bow_mentioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_counts_mentioned           = count_vec_mentioned.fit_transform(X_mentioned_bow_pd)\n",
    "X_tfidf_mentioned            = tfidf_transformer_mentioned.fit_transform(X_counts_mentioned)\n",
    "\n",
    "X_test_mentioned_counts      = count_vec_mentioned.transform(X_test_mentioned_bow_pd)\n",
    "X_test_mentioned_tfidf       = tfidf_transformer_mentioned.transform(X_test_mentioned_counts)\n",
    "\n",
    "lf_mentioned = LogisticRegression(C=C_optimal_mentioned)\n",
    "lf_mentioned.fit(X_tfidf_mentioned, Y_mentioned_bow_pd)\n",
    "\n",
    "Y_mentioned_bow_predicted_test = lf_mentioned.predict(X_test_mentioned_tfidf)\n",
    "\n",
    "#print(Y_mentioned_bow_predicted_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<447x792 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 405 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_mentioned_tfidf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = []\n",
    "for i in range(X.shape[0]):\n",
    "    X_all.append(X[i].split())\n",
    "    \n",
    "for i in range(X_test.shape[0]):\n",
    "    X_all.append(X_test[i].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = all_tweets.tolist()\n",
    "words  = \" \".join(tweets)\n",
    "words  = \" \".join(words.split(\"\\n\")) \n",
    "words  = \" \".join(words.split(\"\\t\"))\n",
    "words  = \" \".join(words.split(\"\\xa0\")).split()\n",
    "whole_text = \" \".join(words)\n",
    "\n",
    "count          = collections.Counter(words)\n",
    "count_clean    = [(item, count[item]) for item in count if count[item]>1]\n",
    "vocab_size     = len(count_clean)\n",
    "num_words      = vocab_size+2 \n",
    "\n",
    "dic = {}\n",
    "\n",
    "for i in range(vocab_size):\n",
    "    dic[count_clean[i][0]] = (i+1, count_clean[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_token_list(A):\n",
    "    B      = A.strip().split(\" \")\n",
    "    output = []\n",
    "    for item in B:\n",
    "        if item in dic:\n",
    "            output.append(dic[item][0])\n",
    "        else:\n",
    "            output.append(vocab_size + 1)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tokens       = X.apply(lambda x: string_to_token_list(x))\n",
    "X_test_tokens  = X_test.apply(lambda x: string_to_token_list(x)) \n",
    "\n",
    "num_tokens = [len(tokens) for tokens in X_tokens]\n",
    "num_tokens = np.array(num_tokens)\n",
    "max_tokens = int(np.max(num_tokens))\n",
    "\n",
    "X_pad       = []\n",
    "X_test_pad  = []\n",
    "\n",
    "for item in X_tokens:\n",
    "    n = len(item)\n",
    "    X_pad.append([0]*(max_tokens-n) + item)\n",
    "\n",
    "for item in X_test_tokens:\n",
    "    n = len(item)\n",
    "    X_test_pad.append([0]*(max_tokens-n) + item)\n",
    "    \n",
    "X_pad        = np.array(X_pad)\n",
    "X_test_pad   = np.array(X_test_pad)\n",
    "X_all_pad    = np.concatenate((X_pad , X_test_pad), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pad, X_val_pad, Y_train_pad, Y_val_pad = train_test_split(X_pad, Y, test_size = 0.1, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameter selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr                  = 1e-4\n",
    "batch_size          = 64\n",
    "embedding_size      = 128\n",
    "lstm_out            = 32\n",
    "lstm_dropout_prob   = 0.5\n",
    "lstm_forget_bias    = 1.0\n",
    "hidden_layer_size_1 = 32\n",
    "hidden_layer_size_2 = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(X_all, iter=10, min_count=2, size=embedding_size, workers=4)\n",
    "#words = list(model.wv.vocab)\n",
    "#print(words)\n",
    "#print(model.wv.similar_by_word('makeamericagreatagain'))\n",
    "#model.wv['trump'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((1,embedding_size), np.float32)\n",
    "\n",
    "for i in range(vocab_size):\n",
    "    word             = count_clean[i][0]\n",
    "    word_vec         = model.wv[word]\n",
    "    embedding_matrix = np.vstack((embedding_matrix, word_vec))\n",
    "\n",
    "embedding_matrix = np.vstack((embedding_matrix, 5* np.ones((1,embedding_size), np.float32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test_data_set(filename, X_test_pad, sess):\n",
    "        test_size   = X_test_pad.shape[0]\n",
    "        test_remain = batch_size - (test_size % batch_size) \n",
    "        X_test_pad  = np.concatenate((X_test_pad, np.zeros((test_remain, max_tokens), np.float32)), axis=0)\n",
    "        m = test_size // batch_size \n",
    "        pred_test_vals = np.empty((0, 2), np.float32)\n",
    "\n",
    "        for i in range(m+1):\n",
    "            input_test_batch = X_test_pad[i * batch_size : (i+1) * batch_size, :]\n",
    "            test_preds_list  = sess.run([predictions], feed_dict = {X : input_test_batch})\n",
    "            pred_test_batch  = np.asarray(test_preds_list).reshape(batch_size, 2)\n",
    "            pred_test_vals   = np.concatenate((pred_test_vals, pred_test_batch), axis=0)\n",
    "                 \n",
    "            \n",
    "        with open(filename,\"w+\") as outputfile:\n",
    "            outputfile.write(\"id,realDonaldTrump,HillaryClinton\\n\")\n",
    "            for j in range(test_size):\n",
    "                hillary = pred_test_vals[j][1]\n",
    "                donald  = pred_test_vals[j][0]\n",
    "                outputfile.write(str(j)+\",\"+str(donald)+\",\"+str(hillary)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(X, Y, size):\n",
    "    rdm      = np.random.choice(X.shape[0], size , replace = False)\n",
    "    y1       = Y[rdm].reshape((size,1))\n",
    "    y2       = (y1+1)%2\n",
    "    Y_out    = np.concatenate((y1, y2), axis=1)\n",
    "    X_out    = X[rdm,:]\n",
    "    return X_out, Y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at \n",
    "https://github.com/bernhard2202/twitter-sentiment-analysis/blob/master/model/lstm.py\n",
    "\n",
    "For a similar implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/cpu:0\"):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(0)\n",
    "\n",
    "    X = tf.placeholder(tf.int32, shape=[batch_size, max_tokens])\n",
    "    Y = tf.placeholder(tf.float32, shape=[batch_size,2])\n",
    "        \n",
    "    embeddings     = tf.Variable(embedding_matrix)    \n",
    "    embed          = tf.nn.embedding_lookup(embeddings, X)\n",
    "    embed          = tf.unstack(embed, max_tokens, 1)\n",
    "    \n",
    "    lstm_cell                 = tf.contrib.rnn.BasicLSTMCell(lstm_out, forget_bias = lstm_forget_bias)\n",
    "    lstm_cell                 = tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=lstm_dropout_prob)\n",
    "    lstm_output , lstm_state  = tf.contrib.rnn.static_rnn(lstm_cell, inputs=embed , dtype=tf.float32)\n",
    "    \n",
    "    outputs                   = tf.stack(lstm_output)\n",
    "    outputs                   = tf.gather(outputs, max_tokens-1, axis=0)\n",
    "    outputs                   = tf.reshape(outputs, [batch_size, lstm_out])\n",
    "    lstm_final_output         = tf.reshape(outputs, [batch_size , lstm_out])\n",
    "    hidden_layer_1_output     = tf.layers.dense(lstm_final_output, hidden_layer_size_1)\n",
    "    hidden_layer_2_output     = tf.layers.dense(hidden_layer_1_output, hidden_layer_size_2)\n",
    "    \n",
    "    out_weight      = tf.Variable(tf.random_normal([hidden_layer_size_2, 2]))\n",
    "    out_bias        = tf.Variable(tf.random_normal([2]))\n",
    "    \n",
    "    scores          = tf.nn.xw_plus_b(hidden_layer_2_output, out_weight,out_bias)\n",
    "    predictions     = tf.nn.softmax(scores)\n",
    "    \n",
    "    losses          = tf.nn.softmax_cross_entropy_with_logits_v2(logits = scores,\n",
    "                                                              labels = Y)\n",
    "    loss            = tf.reduce_mean(losses)\n",
    "    \n",
    "    correct_pred    = tf.equal(tf.argmax(predictions, 1), tf.argmax(Y, 1))\n",
    "    accuracy        = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    optimizer       = tf.train.AdamOptimizer(lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "epochs           = 1500\n",
    "display_epoch    = 100\n",
    "\n",
    "loss_val_vec     = []\n",
    "accur_val_vec    = []\n",
    "loss_train_vec   = []\n",
    "accur_train_vec  = []\n",
    "epoch_vec = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            batch_inputs, batch_labels = get_batch(X_train_pad, Y_train_pad, batch_size)\n",
    "            loss_train, accur_train, _ = sess.run([loss, accuracy, optimizer],\n",
    "                                                  feed_dict = {X : batch_inputs,\n",
    "                                                               Y : batch_labels})\n",
    "            \n",
    "            batch_inputs, batch_labels = get_batch(X_val_pad, Y_val_pad, batch_size)\n",
    "            loss_val, accur_val = sess.run([loss, accuracy],\n",
    "                                           feed_dict = {X : batch_inputs,\n",
    "                                                        Y : batch_labels})\n",
    "                \n",
    "            loss_train_vec.append(loss_train)\n",
    "            accur_train_vec.append(accur_train)\n",
    "            loss_val_vec.append(loss_val)\n",
    "            accur_val_vec.append(accur_val)\n",
    "            epoch_vec.append(epoch)\n",
    "            \n",
    "            if epoch % display_epoch == 0:\n",
    "                print(\"Epoch \" +str(epoch) + \", Batch loss = \", \n",
    "                      \"%.3f, \" % loss_train , \"training accuray = \",  \"%.3f, \" % accur_train, \"val loss = \",\n",
    "                      \"%.3f, \" % loss_val , \"val accuray = \",  \"%.3f, \" % accur_val)\n",
    "        \n",
    "        evaluate_test_data_set(\"/Users/mohsenkiskani/Downloads/submissions/submission_lstm_results.csv\",\n",
    "                               X_test_pad, sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig1, (ay1, ay2) = plt.subplots(2, 1)\n",
    "ay1.plot(epoch_vec, loss_train_vec)\n",
    "ay2.plot(epoch_vec, loss_val_vec)\n",
    "\n",
    "fig2, (ay3, ay4) = plt.subplots(2, 1)\n",
    "ay3.plot(epoch_vec, accur_train_vec)\n",
    "ay4.plot(epoch_vec, accur_val_vec)\n",
    "\n",
    "ay1.set_title('Training batch loss vs epoch')\n",
    "ay1.set_xlabel('Epoch')\n",
    "ay1.set_ylabel('Training batch loss')\n",
    "\n",
    "ay2.set_title('Validation batch loss vs epoch')\n",
    "ay2.set_xlabel('Epoch')\n",
    "ay2.set_ylabel('Validation batch loss')\n",
    "\n",
    "ay3.set_title('Training batch accuracy vs epoch')\n",
    "ay3.set_xlabel('Epoch')\n",
    "ay3.set_ylabel('Train accuracy')\n",
    "\n",
    "ay4.set_title('Validation batch accuracy vs epoch')\n",
    "ay4.set_xlabel('Epoch')\n",
    "ay4.set_ylabel('Validation accuracy')\n",
    "\n",
    "fig1.set_size_inches(18.5, 10.5)\n",
    "fig1.savefig('loss.png', dpi=100)\n",
    "\n",
    "fig2.set_size_inches(18.5, 10.5)\n",
    "fig2.savefig('loss.png', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
